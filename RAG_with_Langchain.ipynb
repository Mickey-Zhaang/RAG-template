{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40c7784",
   "metadata": {},
   "source": [
    "# Basics of RAG from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eccabe",
   "metadata": {},
   "source": [
    "Install and import all requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5767368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.65.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.10)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.22 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.2.23)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.1.93)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3.0,>=0.2.22->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\micke\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.22->langchain) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\micke\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install openai langchain faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef0ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4b109",
   "metadata": {},
   "source": [
    "Get the data you want your LLM to be aware of when answer inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b7c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_str(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple file reader that loads data into string format\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        docs = f.read()\n",
    "    return docs\n",
    "\n",
    "# loads the file in as a str\n",
    "data = load_str(\"what_is_rag.txt\") # (optional) print to see if you get the right data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d12b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # length of each chunk by len() -> so linke a length 1000 str\n",
    "    chunk_overlap=200, # allowed overlap, so chunk 1 could be 1-1000, and chunk 2 be 800-1800 -> overflow of context\n",
    "    separators=[\":\"] # what we should try to chunk by\n",
    ")\n",
    "\n",
    "# splits text up into chunks\n",
    "docs = text_splitter.split_text(data) # (optional) print and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your OpenAI key\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Instantiate a client that answers the questions\n",
    "client = ChatOpenAI(openai_api_key=openai_key, model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Instantiate a client that will turn text into vector embeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_key)\n",
    "\n",
    "# vector stores: stores information through an indexing system -> vectorized representations by using embeddings\n",
    "vector_store = FAISS.from_texts(docs, embedding=embeddings)\n",
    "\n",
    "# the retrieval machine -> takes in the chat bot and the vector store\n",
    "retrieval = RetrievalQA.from_chain_type(\n",
    "    llm=client,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwawrgs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ce0b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement your own chatbot using Retrieval-Augmented Generation (RAG), you can follow these typical workflow steps:\n",
      "\n",
      "1. **Indexing Phase:**\n",
      "   - **Source Documents:** Collect domain-specific documents or FAQs that you want your chatbot to reference.\n",
      "   - **Chunking:** Split these documents into manageable chunks, such as sentences or paragraphs.\n",
      "   - **Embedding Generation:** Use an embedding model to transform each chunk into high-dimensional vectors.\n",
      "   - **Storage:** Store these embeddings in a vector store for efficient retrieval.\n",
      "\n",
      "2. **Query Phase:**\n",
      "   - **User Query:** When a user sends a message to the chatbot, generate an embedding for this query.\n",
      "   - **Retrieval:** Use techniques like k-NN search to find the top-k closest document chunks from your vector store based on similarity to the query embedding.\n",
      "   - **Prompt Construction:** Create a prompt that includes the user query and the retrieved snippets. Ensure the most relevant snippets are placed first and separate context from the query clearly.\n",
      "\n",
      "3. **Model Response Generation:**\n",
      "   - **Pass Prompt to Model:** Feed the constructed prompt into a language generation model (like GPT) to generate a response.\n",
      "   - **Return Response:** Send the generated response back to the user, optionally including citations or references to the source documents used.\n",
      "\n",
      "4. **Considerations:**\n",
      "   - **Latency:** Be aware of the added time for retrieval and prompt construction, which may affect response times.\n",
      "   - **Retrieval Quality:** Ensure that the quality of embeddings and the vector index is high to improve the accuracy of the retrieved documents.\n",
      "   - **Cost and Resources:** Consider the computational resources needed for both embedding generation and model inference.\n",
      "\n",
      "By following these steps, you can create a chatbot that leverages up-to-date information and provides accurate and contextually relevant responses.\n",
      "\n",
      "Chunk 1: Retrieval-Augmented Generation (RAG)  1. Definition Retrieval-Augmented Generation (RAG) is an approach in natural language processing that combines external information retrieval with language model generation. Rather than relying solely on the language model’s pretraining knowledge, RAG fetches relevant documents or data at query time and feeds them to the model, yielding more accurate, up-to-date, and contextually grounded responses.  2. Historical Context - Traditional LLMs generate based on patterns learned during pretraining, which can become stale or hallucinate when asked about niche or newly-emerged topics. - Retrieval-augmented methods emerged to mitigate these issues by providing fresh context from external sources, beginning with research like REALM (Google, 2020) and subsequent studies such as RAG by Facebook AI Research (2021).  3. Core Components a. Embedding Model    - Transforms text into high-dimensional vectors.    - Popular choices…\n",
      "Chunk 2: : structured indices and query interfaces. - Haystack: end-to-end pipelines focusing on production deployment. - OpenAI function-calling: embed retrieval results into structured function calls.  9. Use Cases - Question answering over domain-specific docs (legal, medical, technical). - Chatbots with up-to-date knowledge (company FAQs, product manuals). - Research assistants that cite sources explicitly. - Summarization with background context from recent publications.  10. Benefits - Accuracy: reduces hallucination by grounding in retrieved text. - Freshness: can index new data without retraining the LLM. - Explainability: provides source documents alongside generated answers.  11. Challenges & Limitations - Context Window: limited by the model’s maximum tokens; too many snippets can overflow. - Latency: retrieval and prompt construction add overhead. - Retrieval Quality: depends on embedding quality and vector index performance. - Cost…\n",
      "Chunk 3: : {}\\n Question: {} Answer:\"\"\".format(\"\\n\".join(retrieved), q) response = openai.ChatCompletion.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":prompt}]) print(response.choices[0].message.content) ```…\n",
      "Chunk 4: : “stuff” (concatenate all docs), “map_reduce”, “refine”.  4. Typical Workflow 1. Indexing Phase    - Split source documents into chunks (sentences, paragraphs).    - Generate embeddings for each chunk.    - Store embeddings in a vector store. 2. Query Phase    - Receive a user query.    - Generate an embedding for the query.    - Retrieve top-k closest document chunks from the vector store.    - Construct a prompt that includes the query and retrieved snippets.    - Pass the prompt to the generation model.    - Return the model’s answer, optionally with source citations.  5. Retrieval Techniques - k-NN Search: find k nearest neighbors via cosine similarity or L2 distance. - Hybrid search: combine keyword-based filters with vector similarity. - Filtering: apply metadata or attribute-based filters before retrieval.  6. Prompt Engineering - Order: place the most relevant snippets first. - Formatting: separate context from query with clear delimiters. - Instructions…\n"
     ]
    }
   ],
   "source": [
    "question = \"How would I use RAG to implement my own chat bot?\"\n",
    "\n",
    "# Ask the question\n",
    "response = retrieval.invoke(question)\n",
    "\n",
    "print(response[\"result\"] + \"\\n\")\n",
    "for i, doc in enumerate(response[\"source_documents\"], 1):\n",
    "    snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "    print(f\"Chunk {i}: {snippet}…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
